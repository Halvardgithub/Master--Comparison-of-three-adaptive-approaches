---
title: "CV on real data"
author: "Halvard"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Implementing the adaptive ICARS with rgeneric
Both EW-ICAR and RW-ICAR can easily compute a list of all the taus for the edges. From this it is possible to construct the precision matrix Q, maybe along with the index set as well. I will start by experimenting with the creation of Q before implementing the whole matrix in rgeneric. After the models are defined in rgeneric I will perform cross validation for all the diseases, and further investigate the scarce and prominent cause of death, maybe also total mortality. The comparison of the models will be done for all n, which is always one for Wakefield, and a choice concerning test diseases for lower n, i.e. 10, 20, 50 must be made in regards to using a known disease or not in the training data.

```{r}
library(INLA)
#res_list_testing <- readRDS("Results//result_list_ABYM.rds")

#define nAreas and maybe nDiseases

# constuct tau_list as in the other file, then make Q
# also make index, maybe from carto.wb as in the other file

index2 <- index
index2[1] <- 0

#as a function
constructQ <- function(tau, indexes, nAreas, adj){
  #tau is a list of each edge, 222 elements, indexes keeps track of the indexes for the adj list, which contains all the indexes of neighbors. nAreas is the number of areas.
  Q <- matrix(0, nrow = nAreas, ncol = nAreas)
  count_num <- 1
  for (i in 1:nAreas){
    for (j in adj[(indexes[i]+1):indexes[i+1]]){
      #if (i < j){ #only need to construct the upper half of Q for INLA
        Q[i, j] <- -tau[count_num]
        
        Q[i, i] <- Q[i, i] + tau[count_num]
      #}
      count_num <- count_num + 1
    }
  }
  return(Q)
}

```

## Making the matrices 
Make the matrices for EW-ICAR and RW-ICAR for different number of diseases n.

```{r}
edges_df <- readRDS("Edges_df//Edges_W_scaling_WO_theta")

Q_ABYM10 <- constructQ(edges_df$edge_value_ABYM_10, index2, nAreas, carto.wb$adj)
Q_ABYM20 <- constructQ(edges_df$edge_value_ABYM_20, index2, nAreas, carto.wb$adj)
Q_ABYM50 <- constructQ(edges_df$edge_value_ABYM_50, index2, nAreas, carto.wb$adj)
Q_ABYM86 <- constructQ(edges_df$edge_value_ABYM_86, index2, nAreas, carto.wb$adj)

Q_NSRW10 <- constructQ(edges_df$edge_value_NSRW1_10, index2, nAreas, carto.wb$adj)
Q_NSRW20 <- constructQ(edges_df$edge_value_NSRW1_20, index2, nAreas, carto.wb$adj)
Q_NSRW50 <- constructQ(edges_df$edge_value_NSRW1_50, index2, nAreas, carto.wb$adj)
Q_NSRW86 <- constructQ(edges_df$edge_value_NSRW1_86, index2, nAreas, carto.wb$adj)
```

## The adaptive ICAR as a rgeneric definiton
The function defining the latent term with a given structure matrix $Q$ as an input. I think this could be the same for each method, simply different structure matrices provided. If they differ, it should only be for priors??

Should I scale with inla.model.scale = T, or scale it manually inside the rgenric definition in the Q() function?? -- I think it must be manual, before scaling with tau.

Is it fine with numeric(0) for the log constant??

```{r}
inla.rgeneric.Adaptive.ICAR = function(
  cmd = c("graph", "Q", "mu", "initial", "log.norm.const","log.prior", "quit"),
  theta = NULL)
{
  #Input:
  #Q as the unscaled precision structure matrix
  #nAreas is the number of provinces
  
  envir = parent.env(environment())
  
  interpret_theta <- function() {return(list(tau = exp(theta[1L])))}
  
  graph <- function() {return(Q())}
  
  Q <- function() {
    p <- interpret_theta()
    
    gv <- exp(1 / nAreas * sum(log(diag(INLA:::inla.ginv(R))))) #the geometric variance
    return(inla.as.sparse(p$tau * gv * R))
  }
  
  # Q <- function() {
  #   R <- toeplitz(c(2, -1, rep(0, N - 2)))# 2 on diag and -1 on firstdiags
  #   R[1, 1] <- R[N, N] <- 1 # 1 for first and last diag element
  #   gv <- exp(1 / N * sum(log(diag(INLA:::inla.ginv(R))))) #the geometric variance
  #   R_star <- gv * R 
  #   
  #   p <- interpret_theta()
  #   Q <- p$tau * R_star
  #   return(inla.as.sparse(Q))
  # }
  
  mu <- function() {return(numeric(0))}
  
  initial <- function() {return(4)}#default for precisions: initial = 4
  
  log.norm.const <- function() {return(numeric(0))} #Inla computes it, 
  
  log.prior <- function() {#default: shape = 1, rate = 0.00005 for tau
    p <- interpret_theta()
    prior <- dgamma(p$tau, shape = 1, rate = 0.00005, log = TRUE) + log(p$tau)
    return(prior)
  }
  
  quit <- function() {return(invisible())}
  
  #to ensure theta is defined
  if (!length(theta)) theta = initial()
  
  vals <- do.call(match.arg(cmd), args = list())
  return(vals)
}

# INLA:::inla.ginv(Q_ABYM10)
# image(Q_ABYM10)
# diag(INLA:::inla.ginv(Q_ABYM10))
 
 # rowSums(Q_ABYM10)
 # Q_ABYM10[1, 8]
 # edges_df$edge_value_ABYM_10[1]

#rowSums(R1 + R2)
#diag(INLA:::inla.ginv(R1 + R2))
 

#The specific models
ABYM10_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_ABYM10, nAreas = nAreas)
ABYM20_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_ABYM20, nAreas = nAreas)
ABYM50_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_ABYM50, nAreas = nAreas)
ABYM86_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_ABYM86, nAreas = nAreas)

NSRW10_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_NSRW10, nAreas = nAreas)
NSRW20_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_NSRW20, nAreas = nAreas)
NSRW50_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_NSRW50, nAreas = nAreas)
NSRW86_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_NSRW86, nAreas = nAreas)
```

Then also create the structure matrices R1 and R2 for the BW-ICAR from Wakefield.

```{r}
R1 <- matrix(0, nrow = nAreas, ncol = nAreas) #non-conflict
R2 <- matrix(0, nrow = nAreas, ncol = nAreas) #conflict

#assume the adjacency matrix is called AMat, then
non_zero_indices <- which(adj_matrix == 1, arr.ind = TRUE)

for (k in seq_len(nrow(non_zero_indices))) {
  i <- non_zero_indices[k, 1]
  j <- non_zero_indices[k, 2]

  if(ss_provinces[["Cod_CCAA"]][i] == ss_provinces[["Cod_CCAA"]][j]){
    R1[i, j] <- -1
    R1[i, i] <- R1[i, i] + 1
  }
  else{
    R2[i, j] <- -1
    R2[i, i] <- R2[i, i] + 1
  }
}

#image(R1+R2)

```


## The Cross-Validation
The idea here is to use the Q-matrices obtained from multivariate analysis for the two WinBUGS models, and univariately for the Wakefield model, not actually quite sure how the Wakefield model will work. They will be used as adaptive ICAR models an fitted to the univariate areal data with INLA, the model will look something like this:
$$
Y_{i} = Poisson(\lambda_i) \\
\log(\lambda_i) = \log(E_i) + \mu + \phi_i \\
\boldsymbol{\phi} \sim IGMRF(\mathbf{0}, \mathbf{Q}) 
$$
with the Q matrix assigned from the fitted models in the file "WinBUGS and the adaptive multivariate models.rmd", and they are constructed from the lists further up in this file. Should maybe also add a sum to zero constraint on the $\phi$, currently only a soft sum to zero constraint from mean = numeric(0) in the rgenric definition.

### The testing
Note that `res$cpo$logcpo` gets the log sum of the individual cpo-values, so taking minus and dividing by nAreas gives the same as `-mean(log(res$cpo$cpo))`.
```{r}

ExpectedCases <- readRDS("ExpectedCasesMiguel.rds")
ObservedCases <- readRDS("ObservedCases.rds")

test_indecies <- 1:86 #some numbers for the diseases to be validated on

criteria_df <- data.frame(matrix(NA, nrow = length(test_indecies), ncol = ((2*4 + 3)*3)))
colnames(criteria_df) <- c("RW_10_WAIC", "RW_20_WAIC", "RW_50_WAIC", "RW_86_WAIC",
                           "EW_10_WAIC", "EW_20_WAIC", "EW_50_WAIC", "EW_86_WAIC",
                           "RW_10_DIC", "RW_20_DIC", "RW_50_DIC", "RW_86_DIC",
                           "EW_10_DIC", "EW_20_DIC", "EW_50_DIC", "EW_86_DIC",
                           "RW_10_CPO", "RW_20_CPO", "RW_50_CPO", "RW_86_CPO",
                           "EW_10_CPO", "EW_20_CPO", "EW_50_CPO", "EW_86_CPO",
                           "BW_WAIC", "BW_DIC", "BW_CPO", "BW_2_WAIC", "BW_2_DIC", "BW_2_CPO",
                           "ICAR_WAIC", "ICAR_DIC", "ICAR_CPO")

#Need some df to store results, ie. WAIC, LS, CPO and such, like above!

t0 <- Sys.time()

for (j in test_indecies){
  Y <- as.vector(ObservedCases[, j])
  E <- as.vector(ExpectedCases[, j])
  
  data_df <- data.frame(Y = Y, E = E, ID = 1:nAreas)
  
  # The RW-ICAR by Miguel
  
  formula_RW_10 <- Y ~ offset(log(E)) + f(ID, model = ABYM10_model) #intercept is automatically included
  formula_RW_20 <- Y ~ offset(log(E)) + f(ID, model = ABYM20_model)
  formula_RW_50 <- Y ~ offset(log(E)) + f(ID, model = ABYM50_model)
  formula_RW_86 <- Y ~ offset(log(E)) + f(ID, model = ABYM86_model)
  
  res_RW_10 <- inla(formula_RW_10, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_RW_20 <- inla(formula_RW_20, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_RW_50 <- inla(formula_RW_50, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_RW_86 <- inla(formula_RW_86, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  
  # The EW-ICAR by Riddervold/Jo
  
  formula_EW_10 <- Y ~ offset(log(E)) + f(ID, model = NSRW10_model)
  formula_EW_20 <- Y ~ offset(log(E)) + f(ID, model = NSRW20_model)
  formula_EW_50 <- Y ~ offset(log(E)) + f(ID, model = NSRW50_model)
  formula_EW_86 <- Y ~ offset(log(E)) + f(ID, model = NSRW86_model)
  
  res_EW_10 <- inla(formula_EW_10, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_EW_20 <- inla(formula_EW_20, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_EW_50 <- inla(formula_EW_50, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  res_EW_86 <- inla(formula_EW_86, family = "poisson", data = data_df,
                    control.compute = list(cpo = T, waic = T, dic = T))
  
  #BW-ICAR by Wakefield
  formula_BW <- Y ~ offset(log(E)) + f(ID, model = Wakefield_model)
  res_BW <- inla(formula_BW, family = "poisson", data = data_df,
                 control.compute = list(cpo = T, waic = T, dic = T))
  #construct BW Q from tau1 and tau2
  #taus <- c(res_BW$summary.hyperpar[, "mean"])
  prec1_marginal <- inla.tmarginal(function(x) exp(x),
                                   res_BW$internal.marginals.hyperpar[[1]])
  prec2_marginal <- inla.tmarginal(function(x) exp(x), 
                                   res_BW$internal.marginals.hyperpar[[2]])
  
  # Compute summary including the mode
  summary_prec1 <- inla.zmarginal(prec1_marginal, silent = TRUE)
  summary_prec2 <- inla.zmarginal(prec2_marginal, silent = TRUE)
  
  # Access mode
  mode_prec1 <- summary_prec1$mean
  mode_prec2 <- summary_prec2$mean
  
  Q_mat <- mode_prec1*R1 + mode_prec2*R2
  BW_2_model <- inla.rgeneric.define(inla.rgeneric.Adaptive.ICAR, R = Q_mat, nAreas = nAreas)
  
  formula_BW_2 <- Y ~ offset(log(E)) + f(ID, model = BW_2_model)
  
  res_BW_2 <- inla(formula_BW_2, family = "poisson", data = data_df,
                 control.compute = list(cpo = T, waic = T, dic = T))
  
  #The standard ICAR
  
  formula_ICAR <- Y ~ offset(log(E)) + 
    f(ID, model = "besag", graph = adj_matrix, scale.model = TRUE)
  
  res_ICAR <- inla(formula_ICAR, family = "poisson", data = data_df,
                   control.compute = list(cpo = T, waic = T, dic = T))
  
  #Writing the results to a dataframe
  vec_waic_dic <- c(res_RW_10$waic$waic, res_RW_20$waic$waic, res_RW_50$waic$waic,
                    res_RW_86$waic$waic, res_EW_10$waic$waic, res_EW_20$waic$waic, 
                    res_EW_50$waic$waic, res_EW_86$waic$waic, res_RW_10$dic$dic, 
                    res_RW_20$dic$dic, res_RW_50$dic$dic, res_RW_86$dic$dic, 
                    res_EW_10$dic$dic, res_EW_20$dic$dic, res_EW_50$dic$dic,
                    res_EW_86$dic$dic)
  vec_cpo <- c(-mean(log(res_RW_10$cpo$cpo)), -mean(log(res_RW_20$cpo$cpo)),
               -mean(log(res_RW_50$cpo$cpo)), -mean(log(res_RW_86$cpo$cpo)),
               -mean(log(res_EW_10$cpo$cpo)), -mean(log(res_EW_20$cpo$cpo)),
               -mean(log(res_EW_50$cpo$cpo)), -mean(log(res_EW_86$cpo$cpo)))
  #-mean(log(res_RW1$cpo$cpo))
  vec_BW <- c(res_BW$waic$waic, res_BW$dic$dic, -mean(log(res_BW$cpo$cpo)),
              res_BW_2$waic$waic, res_BW_2$dic$dic, -mean(log(res_BW_2$cpo$cpo)))
  vec_ICAR <- c(res_ICAR$waic$waic, res_ICAR$dic$dic, -mean(log(res_ICAR$cpo$cpo)))
  
  criteria_df[j,] <- c(vec_waic_dic, vec_cpo, vec_BW, vec_ICAR)
}

#runtimes and saving
CV_runtime <- Sys.time() - t0
CV_runtime

#testing
Y <- as.vector(ObservedCases[, 73])
E <- as.vector(ExpectedCases[, 73])

data_df <- data.frame(Y = Y, E = E, ID = 1:nAreas)

```

```{r}
# save the criteria_df
#saveRDS(criteria_df, file = "CV-Results//CV_no_iidTheta.rds")
```




Presentation:
Short introduction and motivation for disease mapping, map of spain etc. Motivate why the stationarty assumption in ICAR is to strict, and why neighbours are assumed similar.

Then brief introduction of the methods, can show edgeplots for say n=86, and mention that thicker edges indicate that the regions are similar. Maybe mention Madrid or where old tourists live as outliers.

Then some results, maybe box plots for DIC.

Maybe more plots that I can discuss if I have time.

Conclusion?


